{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Data from here: https://www.kaggle.com/datasets/pqbsbk/german-news-dataset\n",
    "news_data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data.dropna()\n",
    "#Load the text from the first k articles\n",
    "k = 1000\n",
    "text = news_data.iloc[:k].text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word level text analysis with spaCy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "#Set the stop_words parameter to filter out words like 'the','of','is',etc.\n",
    "german_stop_words = stopwords.words('german')\n",
    "vectorizer = CountVectorizer(stop_words=german_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make bag of words\n",
    "\n",
    "#Regex expressions and functions for cleaning out words\n",
    "import re\n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "def good_word(word):\n",
    "    if len(word) >= 3 and not has_numbers(word):\n",
    "        return True\n",
    "    return False\n",
    "bag_of_words = vectorizer.fit_transform(text)\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word,sum_words[0,idx]) for word,idx in vectorizer.vocabulary_.items() if good_word(word)]\n",
    "words_freq = sorted(words_freq,key=lambda x:x[1],reverse=True)\n",
    "top_words = [tup[0] for tup in words_freq]\n",
    "words_dict = dict(words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "print('Top {} words are:'.format(k))\n",
    "for i in range(k):\n",
    "    print('{}. {} - {} times'.format(i+1,words_freq[i][0],words_freq[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(frequencies=words_dict)\n",
    "plt.figure(figsize=(11,9),facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NLTK and spaCy for lemmatization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Use NLTK to split the corpus up into sentences which we then feed to spaCy in order to determine the POS and get better lemmatization results. The goal of lemmatization is breaking down the number of used words even further by not including all the conjugations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#Get all sentences from text\n",
    "combined_text = '\\n'.join(text)\n",
    "sents = sent_tokenize(combined_text,language='german')\n",
    "#Make all sentences lower case\n",
    "sents = [sent.lower().replace('\\n',' ') for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def good_lemma(word):\n",
    "    if word.is_alpha and word.pos_ != \"PROPN\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#For every sentence, lemmatize it and get the POS for each token and make a tuple along with the original token\n",
    "nlp_sents = [[(word,word.lemma_,word.pos_) for word in nlp(sent) if good_lemma(word)] for sent in tqdm(sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Space seperate all the lemmatized tokens\n",
    "lemma_sents = [' '.join([tup[1] for tup in sent]) for sent in nlp_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(lemma_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make bag of words\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "lemma_words_freq = [(word,sum_words[0,idx]) for word,idx in vectorizer.vocabulary_.items() if good_word(word)]\n",
    "lemma_words_freq = sorted(lemma_words_freq,key=lambda x:x[1],reverse=True)\n",
    "lemma_top_words = [tup[0] for tup in lemma_words_freq]\n",
    "lemma_words_dict = dict(lemma_words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "print('Top {} words are:'.format(k))\n",
    "for i in range(k):\n",
    "    print('{}. {} - {} times'.format(i+1,lemma_words_freq[i][0],lemma_words_freq[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(frequencies=lemma_words_dict)\n",
    "plt.figure(figsize=(11,9),facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data for plot\n",
    "import numpy as np\n",
    "lemma_words_counts = [word[1] for word in lemma_words_freq]\n",
    "total_lemma_words = sum(lemma_words_counts)\n",
    "\n",
    "normal_words_counts = [word[1] for word in words_freq]\n",
    "total_words = sum(normal_words_counts)\n",
    "\n",
    "x = np.arange(0,12000)\n",
    "lemma_y = [100*round(sum(lemma_words_counts[:i])/total_lemma_words,3) for i in x]\n",
    "word_y = [100*round(sum(normal_words_counts[:i])/total_words,3) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with plt.style.context('bmh'):\n",
    "    plt.plot(x,lemma_y,color='blue',label='Lemma')\n",
    "    plt.plot(x,word_y,color='red',label='Word')\n",
    "    plt.title(\"Corpus percentage vs. words known\")\n",
    "    plt.xlabel(\"Known words\")\n",
    "    plt.ylabel(\"Percentage of corpus\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Study POS tags</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_if_key_not_exist(dict_obj, key):\n",
    "    \"\"\" Add new key-value pair to dictionary only if\n",
    "    key does not exist in dictionary. \"\"\"\n",
    "    if key not in dict_obj:\n",
    "        dict_obj.update({key: 1})\n",
    "    else:\n",
    "        dict_obj[key] += 1\n",
    "        \n",
    "pos_count_dic = {}\n",
    "for sent in nlp_sents:\n",
    "    for tup in sent:\n",
    "        pos = tup[2]\n",
    "        if pos not in pos_count_dic:\n",
    "            #Add lemma with a count of 1 to dictionary of POS tag/word counts for the current POS\n",
    "            pos_count_dic[pos] = {tup[1]:1}\n",
    "        else:\n",
    "            add_if_key_not_exist(pos_count_dic[pos],tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn into a pandas dataframe by adding a pos column\n",
    "pos_frame = pd.DataFrame()\n",
    "for pos in pos_count_dic.keys():\n",
    "    new_frame = pd.DataFrame.from_dict(pos_count_dic[pos],orient='index',columns=['num'])\n",
    "    new_frame['pos'] = pos\n",
    "    pos_frame = pos_frame.append(new_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of times each POS occurred\n",
    "pos_counts = pos_frame.groupby('pos').num.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a pie chart with \n",
    "import seaborn as sns\n",
    "n = 9\n",
    "data = list(pos_counts.values[:n]) + [sum(pos_counts.values[n:])]\n",
    "labels = list(pos_counts.index)[:n]\n",
    "labels = [val.lower() for val in labels] + ['other']\n",
    "\n",
    "with plt.style.context('bmh'):\n",
    "    colors = sns.color_palette('deep')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    percent = 100*(data/sum(data))\n",
    "    patches, texts = plt.pie(data, colors=colors, startangle=90, radius=1.2)\n",
    "    centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(labels, percent)]\n",
    "    sort_legend = True\n",
    "    if sort_legend:\n",
    "        patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                              key=lambda x: x[2],\n",
    "                                              reverse=True))\n",
    "\n",
    "    plt.legend(patches, labels, loc='best', bbox_to_anchor=(-0.1, 1.),\n",
    "               fontsize=12)\n",
    "    plt.title('German News POS Piechart')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = 5000\n",
    "\n",
    "def get_pos_percentage(freqs,x_range):\n",
    "    \"\"\"\n",
    "    Return a list of percentages representing how each number of known words account for out of all\n",
    "    the words in the count_list.\n",
    "    \"\"\"\n",
    "    total_words = sum(freqs)\n",
    "    x = np.arange(0,x_range)\n",
    "    vals = [100*round(sum(freqs[:i])/total_words,3) for i in x]\n",
    "    return vals\n",
    "\n",
    "pos_tags = ['NOUN','ADV','VERB','ADJ']\n",
    "pos_vals = []\n",
    "for tag in pos_tags:\n",
    "    freqs = pos_frame[pos_frame.pos == tag]['num'].sort_values(ascending=False).values\n",
    "    pos_val = get_pos_percentage(freqs,x_range)\n",
    "    pos_vals.append(pos_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x = np.arange(0,x_range)\n",
    "with plt.style.context('bmh'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    #colors = ['red','green','blue','orange']\n",
    "    #colors = ['#a6cee3','#33a02c','#fb9a99','#e31a1c']\n",
    "    colors = sns.color_palette('deep')\n",
    "    for i,tag in enumerate(pos_tags[:5]):\n",
    "        plt.plot(x,pos_vals[i],color=colors[i],label=tag)\n",
    "    plt.title(\"Percentage of Corpus accounted for vs. known words by POS\")\n",
    "    plt.xlabel(\"Known words\")\n",
    "    plt.ylabel(\"Percentage of corpus\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
